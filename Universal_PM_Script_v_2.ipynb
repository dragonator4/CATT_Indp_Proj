{
 "metadata": {
  "name": "",
  "signature": "sha256:bb06706ed14810ea815ed5c55ee67e57c5e644a42b4f06536d73c08b941e2993"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "from itertools import chain"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#File Paths\n",
      "'''\n",
      "    Needs to be made such that only base path, and folder structure is given.\n",
      "    Example:\n",
      "        Base_Path = 'C:/Projects/APM/Priority Corridors' #Base path, where everything will be found\n",
      "        Corridors = ['MD3', 'MD355'] #List of names of the folders. One for each corridor\n",
      "        TMC_File_Name = ['TMC_Identification.csv'] #Either a list of individual file names, or a common file name\n",
      "        Years = [2012, 2013] #List of years for which computations need to be performed (also name of the folder in which data for that year only will be found)\n",
      "                             #Assumed that all csv files within the year folder contain only the data to be processed. Script will attempt to concatenate multiple csv files in the folder.\n",
      "    \n",
      "    Everything else should be handled by the code. The code should pick the correct TMC file, and the correct data files. If multiple\n",
      "    data files are found within the same year folder, they should be concatenated to create one file.\n",
      "'''\n",
      "Data_Path = 'C:/Projects/APM/Priority Corridors/MD-3/2013/2013_MD_3_VPP.csv'\n",
      "TMC_Path = 'C:/Projects/APM/Priority Corridors/MD-3/TMC_Identification.csv'\n",
      "\n",
      "#Days to be included in computations. General practice is to include only data collected on Tuesday, Wednesday and Thursday.\n",
      "days = [1, 2, 3] #Monday = 0, Sunday = 6\n",
      "\n",
      "#The hours to be included in computations. First sublist will always be considered as freeflow hour.\n",
      "Interest_Periods = [[0], [7, 8], [16, 17]] #For freeflow speed estimation, 7-9 Morning peak, 16-18 Evening peak.\n",
      "\n",
      "#Columns structure of the data files in a dictionary\n",
      "Data_Columns = {'VPP' : ['tmc', 'tstamp', 'speed', 'avg_speed', 'ref_speed', 'tt_min', 'conf', 'cval'],\n",
      "                'NPMRDS' : ['tmc', 'date', 'epoch', 'ttav', 'ttpc', 'ttft']}\n",
      "\n",
      "#Columns for which measures need to be computed in a dictionary. Must be a subset of Columns above.\n",
      "Interest_Columns = {'VPP' : ['speed', 'avg_speed'],\n",
      "                    'NPMRDS' : ['ttav', 'ttpc', 'ttft']}\n",
      "\n",
      "#Data Type. Distinct value for each column in Interest_Columns is required (this would enable mixed data types)\n",
      "Data_Type = {'VPP' : ['speed']*2,    #For Speeds, enter 'speed'.\n",
      "             'NPMRDS' : ['tt']*3}    #For Travel Times, enter 'tt'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Find_Source(df=pd.read_csv(Data_Path, nrows=10, encoding='latin1')):\n",
      "    for i in range(len(Data_Columns)):\n",
      "        if(len(df.columns) == len(list(Data_Columns.values())[i])):\n",
      "            return list(Data_Columns.keys())[i]\n",
      "    else:\n",
      "        raise ValueError('Data did not match any column format specified in Data_Columns dictionary.')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def TMC_Reader(source, tmc_table=pd.read_csv(TMC_Path, encoding='latin1')):\n",
      "    if(len(tmc_table.columns) == len(TMC_Columns[source])):\n",
      "        tmc_table.columns = TMC_Columns[source]\n",
      "        tmc_table.index = tmc_table['tmc']\n",
      "        return tmc_table.drop(tmc_table.columns, 1, inplace=True)\n",
      "    else:\n",
      "        raise ValueError('TMC file did not match with dictionary specification for source: {0}'.format(source))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Data_Reader(chunk, source):\n",
      "    if(source == 'VPP'):\n",
      "        chunk.columns = Data_Columns[source]\n",
      "        chunk['tstamp'] = pd.to_datetime(chunk['tstamp'], format='%Y-%m-%d %H:%M:%S')\n",
      "        temp = pd.DatetimeIndex(chunk['tstamp'])\n",
      "        chunk['weekday'] = temp.weekday\n",
      "        chunk['hour'] = temp.hour\n",
      "        chunk['doy'] = temp.dayofyear\n",
      "        return chunk\n",
      "    elif(source == 'NPMRDS'):\n",
      "        chunk.columns =  Data_Columns[source]\n",
      "        chunk['tstamp'] = pd.to_datetime(df['date'], format='%m%d%Y', box=False) + pd.to_timedelta(df['epoch']*5*60, unit='s')\n",
      "        temp = pd.DatetimeIndex(chunk['tstamp'])\n",
      "        chunk['weekday'] = temp.weekday\n",
      "        chunk['hour'] = temp.hour\n",
      "        chunk['doy'] = temp.dayofyear\n",
      "        return chunk\n",
      "    else:\n",
      "        raise ValueError(\"Don't know how to parse datetime for input data.\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "source = Find_Source()\n",
      "print('Recognized that the source of the input data file is {0}'.format(source))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reading_sums = TMC_Reader(source=source)\n",
      "reading_counts = TMC_Reader(source=source)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv(Data_Path, chunksize=600000)\n",
      "for chunk in df:\n",
      "    chunk = Data_Reader(chunk, source)\n",
      "\n",
      "    chunk = chunk[chunk['weekday'].isin(days)]\n",
      "    hours = list(chain.from_iterable(Interest_Periods))\n",
      "    chunk = chunk[chunk['hour'].isin(hours)]\n",
      "\n",
      "    grouped = chunk.groupby(['tmc', 'doy', 'hour'], sort=False)\n",
      "    sums = grouped[Interest_Columns[source]].sum().unstack(level='hour')\n",
      "    counts = grouped[Interest_Columns[source]].count().unstack(level='hour')\n",
      "    sums.columns.names = ['metric', 'hour']\n",
      "    counts.columns.names = ['metric', 'hour']\n",
      "    \n",
      "    reading_sums = reading_sums.combineAdd(sums.stack(level='metric'))\n",
      "    reading_counts = reading_counts.combineAdd(counts.stack(level='metric'))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reading_sums[reading_sums == 0] = np.nan\n",
      "reading_counts[reading_counts == 0] = np.nan"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for hours in Interest_Periods:\n",
      "    reading_sums[str(hours[0]) + '-' + str(hours[-1] + 1)] = reading_sums[hours].sum(axis=1)\n",
      "    reading_counts[str(hours[0]) + '-' + str(hours[-1] + 1)] = reading_counts[hours].sum(axis=1)\n",
      "    reading_sums.drop(hours, 1, inplace=True)\n",
      "    reading_counts.drop(hours, 1, inplace=True)\n",
      "\n",
      "reading_sums = reading_sums.stack(level='hour').unstack(level='doy')\n",
      "reading_counts = reading_counts.stack(level='hour').unstack(level='doy')\n",
      "\n",
      "tti_speeds = ((reading_sums.sum(axis=1))/(reading_counts.sum(axis=1))).unstack(level='hour')\n",
      "\n",
      "#Needs to handle mixed speed and travel times. Use indexing\n",
      "pti_speeds = (reading_sums/reading_counts).T.quantile(0.05).unstack(level='hour')\n",
      "pti_speeds.drop(pti_speeds.columns[0], 1, inplace=True)\n",
      "\n",
      "perf_mes = tmc_table.join(tti_speeds, sort=False).join(pti_speeds, lsuffix='_TTI', rsuffix='_PTI', sort=False)\n",
      "perf_mes = (perf_mes[perf_mes.columns[0]]/perf_mes.T).T\n",
      "perf_mes = perf_mes.stack().unstack(level='metric').unstack()\n",
      "perf_mes.columns.names = ['Metric', 'Perf_Meas']\n",
      "perf_mes.index.names = ['TMC']\n",
      "perf_mes[perf_mes < 1] = 1\n",
      "\n",
      "t.append(time.time())\n",
      "\n",
      "'''tmc_inchgs = pd.read_csv('C:/Projects/APM/Priority Corridors/MD-3/TMC_Identification_InChgs.csv', encoding='latin1')\n",
      "tmc_inchgs.index = tmc_inchgs['tmc']\n",
      "tmc_inchgs.drop(tmc_inchgs.columns[:-2], 1, inplace=True)\n",
      "\n",
      "perf_mes = perf_mes.join(tmc_inchgs, sort=False)\n",
      "\n",
      "t.append(time.time())\n",
      "\n",
      "perf_mes.to_csv(path[:-4] + '_PM.csv')\n",
      "\n",
      "t.append(time.time())'''\n",
      "for i in range(len(t)):\n",
      "    print(time.strftime('%H:%M:%S', time.localtime(t[i])))\n",
      "\n",
      "print('Done!!!')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}